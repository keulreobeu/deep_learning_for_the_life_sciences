{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "authorship_tag": "ABX9TyPxTAOuAKvKGhVcS8PD2BYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keulreobeu/deep_learning_for_the_life_sciences/blob/main/%ED%98%84%EB%AF%B8%EA%B2%BD%EC%9D%84_%EC%9C%84%ED%95%9C_%EB%94%A5%EB%9F%AC%EB%8B%9D_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrQxjrMk7Qd7",
        "outputId": "2795e105-7a17-4bd2-c586-4c793ccc0fcf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre deepchem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uji16CeWoeNb",
        "outputId": "f0cec9c6-50f3-453a-bc89-1947563996a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.11/dist-packages (2.8.1.dev20250829211407)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.5.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deepchem) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.6.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.13.1)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.15.3)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (from deepchem) (2025.3.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deepchem) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deepchem) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deepchem) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.17.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit->deepchem) (11.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->deepchem) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->deepchem) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "런타임 재시작 후 다시실행\n"
      ],
      "metadata": {
        "id": "I4rDyzi4_vFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"keras<3\" \"tensorflow<2.16\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcTvyGu094kD",
        "outputId": "584f5d53-9ac1-481e-ee55-f8e755fc9d28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras<3 in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow<2.16 in /usr/local/lib/python3.11/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.14.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.16) (2.15.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16) (2025.7.14)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16) (0.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.16) (0.45.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "런타임 재시작 후 다시실행"
      ],
      "metadata": {
        "id": "uEcgnCMnEivf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "564VOYaH7TMq",
        "outputId": "86ade584-06be-415e-a94b-7ab8a56913ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | egrep 'deepchem|rdkit|tensorflow|keras|numpy|pandas|scikit-learn|matplotlib'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-DFVOA5aLuP",
        "outputId": "1178d731-34f5-4b63-adfb-52721e5c834b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deepchem                              2.8.1.dev20250829211407\n",
            "geopandas                             1.0.1\n",
            "keras                                 2.15.0\n",
            "keras-hub                             0.18.1\n",
            "keras-nlp                             0.18.1\n",
            "matplotlib                            3.10.0\n",
            "matplotlib-inline                     0.1.7\n",
            "matplotlib-venn                       1.1.2\n",
            "numpy                                 1.26.4\n",
            "pandas                                2.2.2\n",
            "pandas-datareader                     0.10.0\n",
            "pandas-gbq                            0.29.2\n",
            "pandas-stubs                          2.2.2.240909\n",
            "rdkit                                 2025.3.6\n",
            "scikit-learn                          1.6.1\n",
            "sklearn-pandas                        2.2.0\n",
            "tensorflow                            2.15.1\n",
            "tensorflow-datasets                   4.9.9\n",
            "tensorflow_decision_forests           1.11.0\n",
            "tensorflow-estimator                  2.15.0\n",
            "tensorflow-hub                        0.16.1\n",
            "tensorflow-io-gcs-filesystem          0.37.1\n",
            "tensorflow-metadata                   1.17.2\n",
            "tensorflow-probability                0.25.0\n",
            "tensorflow-text                       2.18.1\n",
            "tf_keras                              2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deepchem as dc\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LAusRNo1EROm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e59be2-0ae1-40bb-9fbe-e5f355569ff5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for SPS. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for NumAmideBonds. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for NumAtomStereoCenters. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for NumBridgeheadAtoms. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for NumHeterocycles. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for NumSpiroAtoms. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for Phi. Feature removed!\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.11/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 구버전 코드\n",
        "```\n",
        "# 이미지 다운\n",
        "wget https://data.broadinstitute.org/bbbc/BBBC005/BBBC005_v1_images.zip\n",
        "unzip BBBC005_v1_images.zip\n",
        "\n",
        "\n",
        "# 이미지 로드\n",
        "image_dir = 'BBBC005_v1_images'\n",
        "files = []\n",
        "labels = []\n",
        "\n",
        "for f in os.listdir(image_dir):\n",
        "  if f.endswith('.tif'):\n",
        "    files.append(os.path.join(image_dir, f))\n",
        "    labels.append(int(re.findall('_c(.*?)_', f)[0]))\n",
        "\n",
        "loaber = dc.data.ImageLoader()\n",
        "dataset = loaber.featurize(files, np.array(labels))\n",
        "\n",
        "\n",
        "# test, valid 분리\n",
        "splitter = dc.splits.RandomSplitter()\n",
        "train_dataset, valid_dataset, test_dataset = splitter.train_valid_test_split(dataset, seed = 123)\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "iearning_rate = dc.models.tensorgraph.optimizers.ExponentiaDecay(\n",
        "    0.001, 0.9, 250)\n",
        "model = dc.models.TensorGraph(learning_rate = learning_rate, model_dir = 'model')\n",
        "features = layers.Features(shape=(None,))\n",
        "labels = layers.Labels(shape=(None,))\n",
        "\n",
        "for num_outputs in [16, 32, 64, 128, 256]:\n",
        "    prev_layer = layers.Conv2D(num_outputs, kernel_size=5, stride=2, in_layers=prev_layer)\n",
        "\n",
        "outputs = laters.Dens(1, in_layers=prev_layer.Flatten(prev_layer))\n",
        "model.add_outputs(outputs)\n",
        "loss = layers.ReduceSum(layers.L2Loss(in_layers=(outputs, labels)))\n",
        "model.set_loss(loss)\n",
        "\n",
        "\n",
        "# 모델의 결과 가져오기 모델 학습에는 1시간 이상걸림\n",
        "model.restore()\n",
        "\n",
        "\n",
        "# 모델 평가\n",
        "y_pred = model.predict(test_dataset).flatten()\n",
        "print(np.sprt(np.mean((y_pred-test_dataset.y)**2)))\n",
        "```"
      ],
      "metadata": {
        "id": "Ci4vGujXxvQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('런타임 유지')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXgCcG2gS_ZY",
        "outputId": "279b303b-8815-43e3-c8ab-c043ec89718f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "런타임 유지\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 0) 기본 설정 & 드라이브\n",
        "# =========================\n",
        "import os, re, math, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Colab에서만: 주석 해제해서 사용\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 경로 설정 (필요에 맞게 바꿔주세요) ---\n",
        "# 학습된 모델 ZIP을 드라이브에 두었다면, 먼저 Colab에서 압축 해제:\n",
        "# !unzip -o \"/content/drive/MyDrive/microscopy_models.zip\" -d \"/content/drive/MyDrive/\"\n",
        "\n",
        "MODEL_ROOT = \"/content/drive/MyDrive/data/microscopy_models\"   # ZIP을 풀어놓은 루트\n",
        "CKPT_DIR   = os.path.join(MODEL_ROOT, \"model\")         # 분류 체크포인트 폴더 (또는 \"segmentation\")\n",
        "# CKPT_DIR = os.path.join(MODEL_ROOT, \"segmentation\")  # 세그멘테이션 평가 시 이 줄로 변경\n",
        "\n",
        "# BBBC005 이미지 폴더\n",
        "IMAGE_DIR  = \"/content/drive/MyDrive/data/BBBC005_v1_images\"\n",
        "\n",
        "# 이미지 전처리 하이퍼파라미터\n",
        "IMG_SIZE   = 128  # 필요시 96/128/256 등으로 조정\n",
        "\n",
        "# 재현성\n",
        "SEED = 123\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# TF1 호환 모드\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# === 1) 이미지 찾기: .tif/.tiff 대소문자 무시 + 하위폴더 재귀 ===\n",
        "tif_files = []\n",
        "for root, _, files in os.walk(IMAGE_DIR):\n",
        "    for f in files:\n",
        "        if f.lower().endswith((\".tif\", \".tiff\")):\n",
        "            tif_files.append(os.path.join(root, f))\n",
        "\n",
        "print(f\"Discovered {len(tif_files)} TIFF files\")\n",
        "print(\"Sample files:\", [os.path.basename(p) for p in tif_files[:5]])\n",
        "assert len(tif_files) > 0, f\"No TIFF files found under {IMAGE_DIR} (check path or unzip status).\"\n",
        "\n",
        "# === 2) 라벨 파싱: '_c숫자_' 또는 '_C숫자_' 모두 허용 ===\n",
        "def extract_label_from_name(fname: str):\n",
        "    # 예: A01_c3_s1.TIF -> 3  / A01_C2_s4.TIF -> 2\n",
        "    m = re.search(r\"_[cC](\\d+)_\", os.path.basename(fname))\n",
        "    if not m:\n",
        "        raise ValueError(f\"Cannot parse label with pattern '_[cC](\\\\d+)_' from filename: {os.path.basename(fname)}\")\n",
        "    return int(m.group(1))\n",
        "\n",
        "labels = [extract_label_from_name(p) for p in tif_files]\n",
        "classes = sorted(set(labels))\n",
        "num_classes = len(classes)\n",
        "print(f\"Classes: {classes} (num_classes={num_classes})\")\n",
        "\n",
        "# === 3) 이미지 로딩/전처리 (그레이스케일 → 리사이즈 → [0,1]) ===\n",
        "IMG_SIZE = 128  # 학습 당시 입력 크기와 맞추세요\n",
        "def load_and_preprocess(path):\n",
        "    img = Image.open(path).convert(\"L\")\n",
        "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "    arr = np.asarray(img, dtype=np.float32) / 255.0\n",
        "    return np.expand_dims(arr, -1)\n",
        "\n",
        "X = np.stack([load_and_preprocess(p) for p in tif_files], axis=0)\n",
        "y = np.array(labels, dtype=np.int32)\n",
        "print(\"X shape:\", X.shape, \" y shape:\", y.shape)\n",
        "\n",
        "# 간단 분할(20% 테스트)\n",
        "X_trv, X_te, y_trv, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y if num_classes > 1 else None\n",
        ")\n",
        "print(\"Test shape:\", X_te.shape)\n",
        "\n",
        "# =========================\n",
        "# 2) TF1 체크포인트 로드 유틸\n",
        "# =========================\n",
        "def latest_ckpt_prefix(ckpt_dir):\n",
        "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        return ckpt.model_checkpoint_path\n",
        "    # fallback: 가장 큰 global_step 가진 prefix 찾기\n",
        "    metas = [f for f in os.listdir(ckpt_dir) if f.endswith(\".meta\")]\n",
        "    assert metas, f\"No .meta found in {ckpt_dir}\"\n",
        "    steps = []\n",
        "    for m in metas:\n",
        "        # e.g., \"model-7700.meta\" -> \"model-7700\"\n",
        "        prefix = m[:-5]\n",
        "        try:\n",
        "            step = int(prefix.split(\"-\")[-1])\n",
        "        except:\n",
        "            step = -1\n",
        "        steps.append((step, prefix))\n",
        "    steps.sort()\n",
        "    return os.path.join(ckpt_dir, steps[-1][1])\n",
        "\n",
        "def list_placeholders_and_outputs(graph):\n",
        "    ops = graph.get_operations()\n",
        "    placeholders = [op.name for op in ops if op.type == \"Placeholder\"]\n",
        "    # 흔한 출력 후보 탐색: Softmax, Sigmoid, Identity, ArgMax 등\n",
        "    candidates = [op.name for op in ops if op.type in (\"Softmax\",\"Sigmoid\",\"Identity\",\"ArgMax\")]\n",
        "    return placeholders, candidates\n",
        "\n",
        "def guess_io_tensors(graph):\n",
        "    placeholders, candidates = list_placeholders_and_outputs(graph)\n",
        "    in_guess = None\n",
        "    # 가장 그럴듯한 입력 플레이스홀더 추정(이름/shape 기준)\n",
        "    for name in placeholders:\n",
        "        try:\n",
        "            t = graph.get_tensor_by_name(name + \":0\")\n",
        "            shp = t.shape.as_list()\n",
        "            if len(shp) == 4:      # (None, H, W, C)\n",
        "                in_guess = name + \":0\"\n",
        "                break\n",
        "        except:\n",
        "            pass\n",
        "    if in_guess is None and placeholders:\n",
        "        in_guess = placeholders[0] + \":0\"\n",
        "\n",
        "    # 출력 후보는 뒤쪽에 나오는 걸 우선\n",
        "    out_guess = None\n",
        "    for name in reversed(candidates):\n",
        "        if \"softmax\" in name.lower() or \"prob\" in name.lower() or \"output\" in name.lower():\n",
        "            out_guess = name + \":0\"\n",
        "            break\n",
        "    if out_guess is None and candidates:\n",
        "        out_guess = candidates[-1] + \":0\"\n",
        "    return in_guess, out_guess, placeholders, candidates\n",
        "\n",
        "# =========================\n",
        "# 3) 복원 & 평가\n",
        "# =========================\n",
        "ckpt_prefix = latest_ckpt_prefix(CKPT_DIR)\n",
        "print(\"Restoring from:\", ckpt_prefix)\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "saver = tf.compat.v1.train.import_meta_graph(ckpt_prefix + \".meta\", clear_devices=True)\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "# 입력/출력 텐서 자동 추정\n",
        "input_name, output_name, phs, outs = guess_io_tensors(graph)\n",
        "print(\"Placeholder candidates (first 10):\", phs[:10])\n",
        "print(\"Output op candidates (last 10):\", outs[-10:])\n",
        "print(\"Auto-guessed INPUT:\", input_name)\n",
        "print(\"Auto-guessed OUTPUT:\", output_name)\n",
        "\n",
        "# 필요시 아래 두 줄을 원하는 텐서 이름으로 \"수동\" 지정해 덮어쓰기 가능\n",
        "# input_name  = \"Features:0\"\n",
        "# output_name = \"Softmax:0\"\n",
        "\n",
        "x_tensor = graph.get_tensor_by_name(input_name)\n",
        "y_tensor = graph.get_tensor_by_name(output_name)\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    saver.restore(sess, ckpt_prefix)\n",
        "\n",
        "    # 배치 처리(메모리 아낄 때)\n",
        "    def predict_in_batches(X, batch=64):\n",
        "        outs = []\n",
        "        for i in range(0, len(X), batch):\n",
        "            outs.append(sess.run(y_tensor, feed_dict={x_tensor: X[i:i+batch]}))\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "    y_pred_raw = predict_in_batches(X_te)\n",
        "\n",
        "# =========================\n",
        "# 4) 성능 지표 출력\n",
        "# =========================\n",
        "if num_classes > 1:\n",
        "    # 분류: 소프트맥스/로짓 모두 대응\n",
        "    if y_pred_raw.ndim == 2 and y_pred_raw.shape[1] > 1:\n",
        "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
        "    else:\n",
        "        # 이진 로짓/시그모이드 같은 경우\n",
        "        y_pred = (y_pred_raw.reshape(-1) >= 0.5).astype(np.int32)\n",
        "    acc = (y_pred == y_te).mean()\n",
        "    print(f\"[CLASSIFICATION] Test Accuracy: {acc:.4f}\")\n",
        "else:\n",
        "    # 회귀\n",
        "    y_pred = y_pred_raw.reshape(-1)\n",
        "    rmse = math.sqrt(np.mean((y_pred - y_te.astype(np.float32))**2))\n",
        "    print(f\"[REGRESSION] Test RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "LCTxyDrJbT77",
        "outputId": "fc4e5694-7cc2-411f-848a-fc16c6c57325"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Discovered 19200 TIFF files\n",
            "Sample files: ['SIMCEPImages_J01_C1_F29_s07_w1.TIF', 'SIMCEPImages_J01_C1_F29_s15_w2.TIF', 'SIMCEPImages_J02_C5_F29_s07_w1.TIF', 'SIMCEPImages_J02_C5_F29_s03_w1.TIF', 'SIMCEPImages_J01_C1_F29_s12_w2.TIF']\n",
            "Classes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 31, 35, 40, 44, 48, 53, 57, 61, 66, 70, 74, 78, 83, 87, 91, 96, 100] (num_classes=42)\n",
            "X shape: (19200, 128, 128, 1)  y shape: (19200,)\n",
            "Test shape: (3840, 128, 128, 1)\n",
            "Restoring from: /content/drive/MyDrive/data/microscopy_models/model/model-7700\n",
            "Placeholder candidates (first 10): ['Placeholder', 'Feature_13_pre_q/Placeholder', 'Label_16_pre_q/Placeholder']\n",
            "Output op candidates (last 10): ['Conv2D_6/bias/Adam_1/read', 'Conv2D_4/kernel/Adam/read', 'Conv2D_4/kernel/Adam_1/read', 'Conv2D_4/bias/Adam/read', 'Conv2D_4/bias/Adam_1/read', 'Dense_1/kernel/Adam/read', 'Dense_1/kernel/Adam_1/read', 'Dense_1/bias/Adam/read', 'Dense_1/bias/Adam_1/read', 'save/control_dependency']\n",
            "Auto-guessed INPUT: Placeholder:0\n",
            "Auto-guessed OUTPUT: save/control_dependency:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot feed value of shape (64, 128, 128, 1) for Tensor Placeholder:0, which has shape ()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-2073873009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0my_pred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12-2073873009.py\u001b[0m in \u001b[0;36mpredict_in_batches\u001b[0;34m(X, batch)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[1;32m   1188\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0;32m-> 1189\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0;34mf'Cannot feed value of shape {str(np_val.shape)} for Tensor '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;34mf'{subfeed_t.name}, which has shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 128, 128, 1) for Tensor Placeholder:0, which has shape ()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 패치: 올바른 입력 텐서를 자동 선택해서 예측/평가 =====\n",
        "import numpy as np, math, tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "def shape_list(t):\n",
        "    try:\n",
        "        return t.shape.as_list()\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def default_extra_feeds(graph):\n",
        "    feeds = {}\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            t = graph.get_tensor_by_name(op.name + \":0\")\n",
        "            shp = t.shape.as_list()\n",
        "            if shp == [] or shp == [None] or shp == ():\n",
        "                name = op.name.lower()\n",
        "                if t.dtype == tf.bool and (\"is_training\" in name or \"training\" in name or \"train\" in name):\n",
        "                    feeds[t] = False\n",
        "                elif t.dtype.is_floating and (\"keep_prob\" in name or \"dropout\" in name or \"drop_rate\" in name):\n",
        "                    feeds[t] = 1.0\n",
        "    return feeds\n",
        "\n",
        "def find_feed_tensor(graph):\n",
        "    # 1) tf.data / Queue 계열 출력 먼저\n",
        "    for op in graph.get_operations():\n",
        "        if op.type in (\"IteratorGetNext\",\"QueueDequeueV2\",\"QueueDequeueManyV2\",\"QueueDequeueUpToV2\",\"QueueDequeue\"):\n",
        "            for t in op.outputs:\n",
        "                shp = shape_list(t)\n",
        "                if shp and len(shp) == 4:\n",
        "                    return t\n",
        "    # 2) 첫 Conv2D의 입력(보통 (N,H,W,C))\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Conv2D\":\n",
        "            t = op.inputs[0]\n",
        "            shp = shape_list(t)\n",
        "            if shp and len(shp) == 4:\n",
        "                return t\n",
        "    # 3) 4D Placeholder\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            t = graph.get_tensor_by_name(op.name + \":0\")\n",
        "            shp = shape_list(t)\n",
        "            if shp and len(shp) == 4:\n",
        "                return t\n",
        "    return None\n",
        "\n",
        "def find_output_tensor(graph):\n",
        "    # Softmax > Sigmoid > Identity(세이버 제외) > BiasAdd\n",
        "    def last(types):\n",
        "        for op in reversed(graph.get_operations()):\n",
        "            if op.type in types:\n",
        "                return graph.get_tensor_by_name(op.name + \":0\")\n",
        "        return None\n",
        "    y = last([\"Softmax\"])\n",
        "    if y is not None: return y\n",
        "    y = last([\"Sigmoid\"])\n",
        "    if y is not None: return y\n",
        "    y = last([\"Identity\"])\n",
        "    if y is not None and \"save\" not in y.name.lower():\n",
        "        return y\n",
        "    return last([\"BiasAdd\"])\n",
        "\n",
        "x_tensor = find_feed_tensor(graph)\n",
        "y_tensor = find_output_tensor(graph)\n",
        "assert x_tensor is not None, \"입력 텐서를 찾지 못했습니다. (Conv2D 입력 또는 QueueDequeue/Iterator 출력 탐색 실패)\"\n",
        "assert y_tensor is not None, \"출력 텐서를 찾지 못했습니다.\"\n",
        "\n",
        "print(\"[Picked INPUT ]\", x_tensor.name, \"shape=\", shape_list(x_tensor))\n",
        "print(\"[Picked OUTPUT]\", y_tensor.name,  \"shape=\", shape_list(y_tensor))\n",
        "\n",
        "extra_feeds = default_extra_feeds(graph)\n",
        "if extra_feeds:\n",
        "    print(\"Extra scalar placeholders auto-fed:\",\n",
        "          {k.name: v for k, v in extra_feeds.items()})\n",
        "\n",
        "def predict_in_batches(sess, X, batch=64):\n",
        "    outs = []\n",
        "    for i in range(0, len(X), batch):\n",
        "        fd = {x_tensor: X[i:i+batch], **extra_feeds}\n",
        "        outs.append(sess.run(y_tensor, feed_dict=fd))\n",
        "    return np.concatenate(outs, axis=0)\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    saver.restore(sess, ckpt_prefix)  # ckpt_prefix는 이전 셀에서 구한 값 사용\n",
        "    y_pred_raw = predict_in_batches(sess, X_te, batch=64)\n",
        "\n",
        "# --- 성능 계산: 출력이 [None,1]이면 회귀로 간주 ---\n",
        "if y_pred_raw.ndim == 2 and y_pred_raw.shape[1] == 1:\n",
        "    y_pred = y_pred_raw.reshape(-1)\n",
        "    rmse = math.sqrt(np.mean((y_pred - y_te.astype(np.float32))**2))\n",
        "    mae  = np.mean(np.abs(y_pred - y_te.astype(np.float32)))\n",
        "    print(f\"[REGRESSION] Test RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "else:\n",
        "    # 다중분류 가정\n",
        "    if y_pred_raw.ndim == 2 and y_pred_raw.shape[1] > 1:\n",
        "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
        "    else:\n",
        "        y_pred = (y_pred_raw.reshape(-1) >= 0.5).astype(np.int32)\n",
        "    acc = (y_pred == y_te).mean()\n",
        "    print(f\"[CLASSIFICATION] Test Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "yMaNEN59gy73",
        "outputId": "062011b4-64c0-44e2-da83-62746a909eec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Picked INPUT ] Conv2D_12/ExpandDims:0 shape= [None, 520, 696, 1]\n",
            "[Picked OUTPUT] Dense_1/Dense_1/BiasAdd:0 shape= [None, 1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot feed value of shape (64, 128, 128, 1) for Tensor Conv2D_12/ExpandDims:0, which has shape (None, 520, 696, 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-2536474360.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_prefix\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ckpt_prefix는 이전 셀에서 구한 값 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0my_pred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# --- 성능 계산: 출력이 [None,1]이면 회귀로 간주 ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-14-2536474360.py\u001b[0m in \u001b[0;36mpredict_in_batches\u001b[0;34m(sess, X, batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_feeds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[1;32m   1188\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0;32m-> 1189\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0;34mf'Cannot feed value of shape {str(np_val.shape)} for Tensor '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;34mf'{subfeed_t.name}, which has shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 128, 128, 1) for Tensor Conv2D_12/ExpandDims:0, which has shape (None, 520, 696, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 교체용: IO 자동탐지 + 평가 ===\n",
        "import math, numpy as np, tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "def tensor_shape_str(t):\n",
        "    try:\n",
        "        return str(t.shape.as_list())\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def list_placeholders(graph):\n",
        "    ph = []\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            t = graph.get_tensor_by_name(op.name + \":0\")\n",
        "            ph.append((op.name, t.dtype.name, t.shape.as_list()))\n",
        "    return ph\n",
        "\n",
        "def pick_input_placeholder(graph, img_size=128):\n",
        "    \"\"\"(N,H,W,C) 형태의 Placeholder 우선, 없으면 2D(플랫텐)도 고려\"\"\"\n",
        "    cands = list_placeholders(graph)\n",
        "    # 1) rank 4 후보\n",
        "    rank4 = [(n,d,s) for (n,d,s) in cands if s is not None and len([x for x in s if x is not None])>=1 and len(s)==4]\n",
        "    # H/W가 img_size와 맞는지 우선 고려\n",
        "    rank4_sorted = sorted(rank4, key=lambda x: (\n",
        "        0 if (x[2][1] in (img_size,None) and x[2][2] in (img_size,None)) else 1\n",
        "    ))\n",
        "    if rank4_sorted:\n",
        "        return rank4_sorted[0][0] + \":0\"\n",
        "    # 2) rank 2 후보(플랫텐)\n",
        "    rank2 = [(n,d,s) for (n,d,s) in cands if s is not None and len(s)==2]\n",
        "    if rank2:\n",
        "        return rank2[0][0] + \":0\"\n",
        "    # 3) 최후: 첫 Placeholder\n",
        "    return cands[0][0] + \":0\" if cands else None\n",
        "\n",
        "def find_output_tensor(graph):\n",
        "    \"\"\"Softmax > Sigmoid > Identity > BiasAdd 순으로 후보 탐색\"\"\"\n",
        "    def last_tensor_by_type(types):\n",
        "        for op in reversed(graph.get_operations()):\n",
        "            if op.type in types:\n",
        "                return graph.get_tensor_by_name(op.name + \":0\")\n",
        "        return None\n",
        "    y = last_tensor_by_type([\"Softmax\"])\n",
        "    if y is not None: return y\n",
        "    y = last_tensor_by_type([\"Sigmoid\"])\n",
        "    if y is not None: return y\n",
        "    # 예측 헤드가 Identity인 경우\n",
        "    y = last_tensor_by_type([\"Identity\"])\n",
        "    if y is not None and \"save\" not in y.name and \"Assign\" not in y.name:\n",
        "        return y\n",
        "    # Dense의 출력은 보통 BiasAdd\n",
        "    y = last_tensor_by_type([\"BiasAdd\"])\n",
        "    return y\n",
        "\n",
        "def default_extra_feeds(graph):\n",
        "    \"\"\"스칼라 플레이스홀더 자동 feed: is_training=False, dropout/keep_prob=1.0 등\"\"\"\n",
        "    feeds = {}\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            t = graph.get_tensor_by_name(op.name + \":0\")\n",
        "            shp = t.shape.as_list()\n",
        "            if shp == [] or shp == [None] or shp == ():\n",
        "                name_low = op.name.lower()\n",
        "                if t.dtype == tf.bool:\n",
        "                    if \"is_training\" in name_low or \"training\" in name_low or \"train\" in name_low:\n",
        "                        feeds[t] = False\n",
        "                elif t.dtype.is_floating:\n",
        "                    if \"keep_prob\" in name_low or \"dropout\" in name_low or \"drop_rate\" in name_low:\n",
        "                        feeds[t] = 1.0\n",
        "                elif t.dtype.is_integer:\n",
        "                    # 학습 단계/글로벌스텝 등은 굳이 feed 안 함\n",
        "                    pass\n",
        "    return feeds\n",
        "\n",
        "# --- 입력/출력 이름 자동 선택 ---\n",
        "input_name = pick_input_placeholder(graph, img_size=IMG_SIZE)\n",
        "y_tensor   = find_output_tensor(graph)\n",
        "assert input_name is not None, \"입력 Placeholder를 찾지 못했습니다.\"\n",
        "assert y_tensor is not None,   \"출력 텐서를 찾지 못했습니다.\"\n",
        "\n",
        "x_tensor = graph.get_tensor_by_name(input_name)\n",
        "\n",
        "# 디버그: 후보 프린트\n",
        "print(\"[Pick] INPUT =\", x_tensor.name, \"shape=\", tensor_shape_str(x_tensor))\n",
        "print(\"[Pick] OUTPUT=\", y_tensor.name,  \"shape=\", tensor_shape_str(y_tensor))\n",
        "\n",
        "# 추가 feed 기본값\n",
        "extra_feeds = default_extra_feeds(graph)\n",
        "if extra_feeds:\n",
        "    print(\"Extra scalar placeholders auto-fed:\", {t.name: v for t,v in extra_feeds.items()})\n",
        "\n",
        "# --- 예측 실행 ---\n",
        "def predict_in_batches(sess, X, batch=64):\n",
        "    outs = []\n",
        "    for i in range(0, len(X), batch):\n",
        "        fd = {x_tensor: X[i:i+batch], **extra_feeds}\n",
        "        outs.append(sess.run(y_tensor, feed_dict=fd))\n",
        "    return np.concatenate(outs, axis=0)\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    saver.restore(sess, ckpt_prefix)\n",
        "    y_pred_raw = predict_in_batches(sess, X_te, batch=64)\n",
        "\n",
        "# --- 성능 계산 ---\n",
        "if num_classes > 1:\n",
        "    # 다중분류: softmax 확률 또는 로짓 가정\n",
        "    if y_pred_raw.ndim == 2 and y_pred_raw.shape[1] > 1:\n",
        "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
        "    else:\n",
        "        # 이진(확률/로짓) 가정\n",
        "        y_pred = (y_pred_raw.reshape(-1) >= 0.5).astype(np.int32)\n",
        "    acc = (y_pred == y_te).mean()\n",
        "    print(f\"[CLASSIFICATION] Test Accuracy: {acc:.4f}\")\n",
        "else:\n",
        "    y_pred = y_pred_raw.reshape(-1)\n",
        "    rmse = math.sqrt(np.mean((y_pred - y_te.astype(np.float32))**2))\n",
        "    print(f\"[REGRESSION] Test RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "dKpKgLEZgQJv",
        "outputId": "6c2f16e7-7f11-4629-8b40-bc59b0e645f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Pick] INPUT = Placeholder:0 shape= []\n",
            "[Pick] OUTPUT= Dense_1/Dense_1/BiasAdd:0 shape= [None, 1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot feed value of shape (64, 128, 128, 1) for Tensor Placeholder:0, which has shape ()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-254823391.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0my_pred_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# --- 성능 계산 ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-13-254823391.py\u001b[0m in \u001b[0;36mpredict_in_batches\u001b[0;34m(sess, X, batch)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_feeds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[1;32m   1188\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0;32m-> 1189\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0;34mf'Cannot feed value of shape {str(np_val.shape)} for Tensor '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;34mf'{subfeed_t.name}, which has shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 128, 128, 1) for Tensor Placeholder:0, which has shape ()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) 복원 & 평가\n",
        "# =========================\n",
        "ckpt_prefix = latest_ckpt_prefix(CKPT_DIR)\n",
        "print(\"Restoring from:\", ckpt_prefix)\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "saver = tf.compat.v1.train.import_meta_graph(ckpt_prefix + \".meta\", clear_devices=True)\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "# 입력/출력 텐서 자동 추정\n",
        "input_name, output_name, phs, outs = guess_io_tensors(graph)\n",
        "print(\"Placeholder candidates (first 10):\", phs[:10])\n",
        "print(\"Output op candidates (last 10):\", outs[-10:])\n",
        "print(\"Auto-guessed INPUT:\", input_name)\n",
        "print(\"Auto-guessed OUTPUT:\", output_name)\n",
        "\n",
        "# 필요시 아래 두 줄을 원하는 텐서 이름으로 \"수동\" 지정해 덮어쓰기 가능\n",
        "# input_name  = \"Features:0\"\n",
        "# output_name = \"Softmax:0\"\n",
        "\n",
        "x_tensor = graph.get_tensor_by_name(input_name)\n",
        "y_tensor = graph.get_tensor_by_name(output_name)\n",
        "\n",
        "with tf.compat.v1.Session(config=config) as sess:\n",
        "    saver.restore(sess, ckpt_prefix)\n",
        "\n",
        "    # 배치 처리(메모리 아낄 때)\n",
        "    def predict_in_batches(X, batch=64):\n",
        "        outs = []\n",
        "        for i in range(0, len(X), batch):\n",
        "            outs.append(sess.run(y_tensor, feed_dict={x_tensor: X[i:i+batch]}))\n",
        "        return np.concatenate(outs, axis=0)\n",
        "\n",
        "    y_pred_raw = predict_in_batches(X_te)\n",
        "\n",
        "# =========================\n",
        "# 4) 성능 지표 출력\n",
        "# =========================\n",
        "if num_classes > 1:\n",
        "    # 분류: 소프트맥스/로짓 모두 대응\n",
        "    if y_pred_raw.ndim == 2 and y_pred_raw.shape[1] > 1:\n",
        "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
        "    else:\n",
        "        # 이진 로짓/시그모이드 같은 경우\n",
        "        y_pred = (y_pred_raw.reshape(-1) >= 0.5).astype(np.int32)\n",
        "    acc = (y_pred == y_te).mean()\n",
        "    print(f\"[CLASSIFICATION] Test Accuracy: {acc:.4f}\")\n",
        "else:\n",
        "    # 회귀\n",
        "    y_pred = y_pred_raw.reshape(-1)\n",
        "    rmse = math.sqrt(np.mean((y_pred - y_te.astype(np.float32))**2))\n",
        "    print(f\"[REGRESSION] Test RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "5xbPhvBnfs5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === A CODE: Inference-only with existing TF1.x checkpoint ===\n",
        "# Colab/TF2.15에서 TF1 그래프 복원하여 추론\n",
        "# - 모델 텐서명을 모를 때를 대비해 자동 탐색 + 요약 프린트\n",
        "# - 입력: (None, 520, 696, 1) float32 [0,1] 정규화\n",
        "# - 라벨: 파일명 \"_C{num}_\"에서 {num}을 정수로 파싱 (세포 수 회귀)\n",
        "# - 출력: RMSE\n",
        "\n",
        "import os, re, glob, json, math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import imageio.v3 as iio\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---- 0) 환경설정: TF1 모드로 전환 & GPU 메모리 증가 허용\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for g in gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ---- 1) 경로 및 하이퍼파라미터\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/data/BBBC005_v1_images\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/data/microscopy_models\"\n",
        "IMG_H, IMG_W, IMG_C = 520, 696, 1\n",
        "\n",
        "# ---- 2) 유틸: 파일명에서 라벨(세포 수) 파싱\n",
        "def parse_label_from_name(fname: str) -> int:\n",
        "    # 예: SIMCEPImages_A01_C1_F1_s11_w2.tif -> 1\n",
        "    m = re.search(r\"_C(\\d+)_\", fname)\n",
        "    if not m:\n",
        "        raise ValueError(f\"파일명에서 라벨(C<num>)을 찾을 수 없음: {fname}\")\n",
        "    return int(m.group(1))\n",
        "\n",
        "# ---- 3) 유틸: TIFF 로더 (흑백 1채널로, 520x696로 리사이즈, [0,1] 스케일)\n",
        "def load_image(path: str) -> np.ndarray:\n",
        "    # Pillow로 로드 (TIFF 지원), L = 8bit grayscale\n",
        "    img = Image.open(path).convert(\"L\")\n",
        "    if (img.size[1], img.size[0]) != (IMG_H, IMG_W):\n",
        "        img = img.resize((IMG_W, IMG_H), resample=Image.BILINEAR)\n",
        "    arr = np.array(img, dtype=np.float32)\n",
        "    arr = arr / 255.0\n",
        "    arr = arr[..., None]  # (H, W, 1)\n",
        "    return arr\n",
        "\n",
        "# ---- 4) 평가용 서브셋 만들기 (예: 무작위 3개 혹은 더 크게)\n",
        "all_files = sorted([os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if f.lower().endswith(\".tif\")])\n",
        "assert len(all_files) > 0, \"이미지 폴더에 .tif 파일이 없습니다.\"\n",
        "# 무작위 3개 샘플 (원하면 n_sample 늘리세요)\n",
        "rng = np.random.default_rng(42)\n",
        "idxs = rng.choice(len(all_files), size=min(64, len(all_files)), replace=False)  # 64장으로 RMSE 한번 계산\n",
        "sample_files = [all_files[i] for i in idxs]\n",
        "\n",
        "X = np.stack([load_image(p) for p in sample_files], axis=0)  # (N, 520, 696, 1)\n",
        "y = np.array([parse_label_from_name(os.path.basename(p)) for p in sample_files], dtype=np.float32)\n",
        "\n",
        "print(\"샘플 배치:\", X.shape, y.shape, \"예) 첫 5개 라벨:\", y[:5])\n",
        "\n",
        "# ---- 5) 체크포인트 탐색 (가장 최신 .meta 기준)\n",
        "meta_list = sorted(glob.glob(os.path.join(MODEL_DIR, \"*.meta\")))\n",
        "if not meta_list:\n",
        "    raise FileNotFoundError(\"모델 디렉터리에 .meta 파일을 찾지 못했습니다.\")\n",
        "meta_path = meta_list[-1]\n",
        "ckpt_prefix = meta_path[:-5]  # .meta 제거\n",
        "print(\"가장 최신 메타그래프:\", meta_path)\n",
        "print(\"체크포인트 prefix:\", ckpt_prefix)\n",
        "\n",
        "# ---- 6) 그래프 로드 & 리스토어\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph())\n",
        "saver = tf.compat.v1.train.import_meta_graph(meta_path, clear_devices=True)\n",
        "saver.restore(sess, ckpt_prefix)\n",
        "\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "# ---- 7) 입력/출력 텐서 자동 추정\n",
        "def find_input_tensor():\n",
        "    # (None, 520, 696, 1) Placeholder를 우선 탐색\n",
        "    candidates = []\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            t = op.outputs[0]\n",
        "            shp = t.shape.as_list()\n",
        "            if len(shp) == 4 and shp[0] is None and shp[1] == IMG_H and shp[2] == IMG_W and shp[3] == IMG_C:\n",
        "                candidates.append(t)\n",
        "    if candidates:\n",
        "        return candidates[0]\n",
        "    # 못찾으면 첫 Placeholder 반환\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            return op.outputs[0]\n",
        "    return None\n",
        "\n",
        "def find_output_tensor():\n",
        "    # 흔한 이름, 컬렉션, 말단 연산 등을 우선 시도\n",
        "    common_names = [\"predictions:0\", \"prediction:0\", \"output:0\", \"logits:0\", \"y_pred:0\", \"y_hat:0\", \"Identity:0\"]\n",
        "    for name in common_names:\n",
        "        try:\n",
        "            t = graph.get_tensor_by_name(name)\n",
        "            return t\n",
        "        except:\n",
        "            pass\n",
        "    # 말단에 가까운 Sigmoid/Identity/Dense 출력 유사 텐서 추정\n",
        "    tail_ops = []\n",
        "    for op in graph.get_operations():\n",
        "        if op.type in (\"Identity\", \"Sigmoid\", \"Relu\", \"Tanh\", \"BiasAdd\", \"Add\", \"MatMul\"):\n",
        "            # 손실/그래디언트 노이즈를 피해 보기 위해 소비자 수가 적은 텐서 선호\n",
        "            if len(op.outputs[0].consumers()) == 0 or all(c.type.startswith(\"Gradients\") for c in op.outputs[0].consumers()):\n",
        "                tail_ops.append(op)\n",
        "    if tail_ops:\n",
        "        # 가장 마지막 생성된 연산의 출력을 후보로\n",
        "        return tail_ops[-1].outputs[0]\n",
        "    # 실패 시 None\n",
        "    return None\n",
        "\n",
        "x_tensor = find_input_tensor()\n",
        "yhat_tensor = find_output_tensor()\n",
        "\n",
        "if x_tensor is None or yhat_tensor is None:\n",
        "    # 디버깅을 위해 그래프 요약 출력\n",
        "    print(\"\\n[디버깅] 그래프에 있는 Placeholder (상위 10개):\")\n",
        "    cnt = 0\n",
        "    for op in graph.get_operations():\n",
        "        if op.type == \"Placeholder\":\n",
        "            print(\" -\", op.name, op.outputs[0].shape)\n",
        "            cnt += 1\n",
        "            if cnt >= 10: break\n",
        "    print(\"\\n[디버깅] 샘플 말단 연산 (상위 20개):\")\n",
        "    tail_ops = [op for op in graph.get_operations() if op.type in (\"Identity\",\"Sigmoid\",\"Relu\",\"Tanh\",\"BiasAdd\",\"Add\",\"MatMul\")]\n",
        "    for op in tail_ops[-20:]:\n",
        "        print(\" -\", op.name, \"->\", op.outputs[0].name, op.outputs[0].shape)\n",
        "    raise RuntimeError(\"입력/출력 텐서를 자동으로 식별하지 못했습니다. 위 요약을 참고해 텐서명을 지정해 주세요.\")\n",
        "\n",
        "print(\"입력 텐서:\", x_tensor.name, x_tensor.shape)\n",
        "print(\"출력 텐서:\", yhat_tensor.name, yhat_tensor.shape)\n",
        "\n",
        "# ---- 8) 추론 & 평가 (RMSE)\n",
        "feed = {x_tensor: X}\n",
        "y_pred = sess.run(yhat_tensor, feed_dict=feed).squeeze()\n",
        "y_pred = np.array(y_pred).reshape(-1)\n",
        "\n",
        "rmse = float(np.sqrt(np.mean((y_pred - y) ** 2)))\n",
        "print(f\"▶ Inference RMSE on {len(y)} images: {rmse:.4f}\")\n",
        "print(\"예측 상위 10개:\", y_pred[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "oOv29viQkqiC",
        "outputId": "51af5964-9b34-49e7-fa56-acc62334566e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 배치: (64, 520, 696, 1) (64,) 예) 첫 5개 라벨: [44. 14. 57. 48. 66.]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "모델 디렉터리에 .meta 파일을 찾지 못했습니다.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-1569058728.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mmeta_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmeta_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"모델 디렉터리에 .meta 파일을 찾지 못했습니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0mmeta_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mckpt_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# .meta 제거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: 모델 디렉터리에 .meta 파일을 찾지 못했습니다."
          ]
        }
      ]
    }
  ]
}